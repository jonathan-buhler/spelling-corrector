{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sized-pathology",
   "metadata": {},
   "source": [
    "# Building a Spelling Corrector\n",
    "\n",
    "Automatic spelling correction is something we encounter every day, but how it works isn't immediately obvious. With this notebook I aim to show how to build an understandable spelling corrector that, as a bonus, also works reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-differential",
   "metadata": {},
   "source": [
    "## First, some probability theory\n",
    "### Equation to maximize\n",
    "Given a misspelled word `w`, we want to create a function `correct(w)` which returns the word the user was most likely trying to type. As there is unavoidable ambiguity in correcting a word (\"ther\" could be \"their\", \"there\", or \"the\", just to name a few) we will assign probabilities to each possible correction (candidate). Each candidate `c`'s probability represents the likelihood that given misspelled word `w`, `c` was the intended word: `P(c|w)`. However, this is a difficult probability to evaluate as there are too many factors involved. Ideally, we want to break down the problem into smaller equations and then combine them.\n",
    "\n",
    "[Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes'_theorem) tells us that `P(c|w)` is equivalent to `(P(c) * P(w|c)) / (P(w)`. `P(w)` is simply the probability of the misspelled word appearing, which is the same for all candidates so we can ignore it as it doesn't factor into the maximization. This leaves us with the equation: `P(c) * P(w|c)` to maximize over all candidates `c` for a misspelled word `w`.\n",
    "\n",
    "That was the most complicated bit of this entire exercise, it's all downhill from here!\n",
    "\n",
    "### Breaking down the equations\n",
    "`P(c)`: The probability of the candidate word occurring in an English text. Words like \"the\", \"and\" etc. occur far more frequently than words like \"monopoly\" or \"horticulture\". This gives them a higher probability of occurring. In other words, you're more likely to be trying to write \"when\" than \"whence\", so P(\"when\") should be higher than P(\"whence\").\n",
    "\n",
    "`P(w|c)`: The probability that the misspelled word `w` was typed when the author meant `c`. For example, `P(\"clasroom\"|\"classroom\")` is very high, while `P(\"cladfalsasrosfsafam\"|\"classroom\")` is very low.\n",
    "\n",
    "Over all candidates `c`: We can't evaluate the equation for every word in the English language, so instead we'll evaluate it on words that are a couple simple edits away from the misspelled word `w`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-supply",
   "metadata": {},
   "source": [
    "## Candidate selection\n",
    "We define a simple edit to a word as any of the following:\n",
    "- Deletion: Remove one letter\n",
    "- Swap: Swap two adjacent letters\n",
    "- Substitution: Change one letter to another\n",
    "- Insertion: Add a new letter\n",
    "\n",
    "This is implemented by the function `one_edit_from`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seven-messenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS = \"abcdefghijklmnopqrstuvwxyz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "backed-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_edit_from(word):\n",
    "    # Generates splits of a word ex. \"dog\" -> [(\"\", \"dog\"), (\"d\", \"og\"), (\"do\", \"g\"), (\"dog\", \"\")]\n",
    "    splits = []\n",
    "    for i in range(len(word) + 1):\n",
    "        split = (word[:i], word[i:])\n",
    "        splits.append(split)\n",
    "\n",
    "    edits = []\n",
    "\n",
    "    # Deletes ex. \"dog\" -> [\"og\", \"dg\", \"do\"]\n",
    "    for (left, right) in splits:\n",
    "        if right:\n",
    "            deleted = left + right[1:]\n",
    "            edits.append(deleted)\n",
    "\n",
    "    # Swaps ex. \"dog\" -> [\"odg\", \"dgo\"]\n",
    "    for (left, right) in splits:\n",
    "        if len(right) > 1:\n",
    "            swapped = left + right[1] + right[0] + right[2:]\n",
    "            edits.append(swapped)\n",
    "\n",
    "    # Substitutes ex. \"dog\" -> [\"aog\", \"dag\", \"doa\",  ...]\n",
    "    for (left, right) in splits:\n",
    "        for sub in LETTERS:\n",
    "            substituted = left + sub + right[1:]\n",
    "            edits.append(substituted)\n",
    "\n",
    "    # Inserts ex. \"cat\" -> [\"acat\", \"caat\", \"caat\", \"cata\", ...]\n",
    "    for (left, right) in splits:\n",
    "        for insert in LETTERS:\n",
    "            inserted = left + insert + right\n",
    "            edits.append(inserted)\n",
    "\n",
    "    # Convert from list to set to remove duplicates\n",
    "    return set(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subsequent-privilege",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "len(one_edit_from(\"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-blink",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['dogc', 'dyg', 'dogt', 'dopg', 'doi', 'dov', 'doq', 'dkg']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "list(one_edit_from(\"dog\"))[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-contamination",
   "metadata": {},
   "source": [
    "This list can get very long. We can handle this by filtering out words that don't exist in the English language. To do this, we create a constant `DICTIONARY` which stores a list of all the words in an English dictionary (read in from a text file). The `utils` functions are included at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decreased-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_dictionary\n",
    "DICTIONARY = get_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abstract-field",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['aah',\n",
       " 'aardvark',\n",
       " 'aardvarks',\n",
       " 'abacus',\n",
       " 'abacuses',\n",
       " 'abalone',\n",
       " 'abalones',\n",
       " 'abandon']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "sorted(list(DICTIONARY))[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-pickup",
   "metadata": {},
   "source": [
    "We can now make the function `filter_unknown` which removes all the words not in `DICTIONARY` from a list of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sublime-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknown(words):\n",
    "    known_words = []\n",
    "    for word in words:\n",
    "        if word in DICTIONARY:\n",
    "            known_words.append(word)\n",
    "    return set(known_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "square-delivery",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'bog',\n",
       " 'cog',\n",
       " 'dag',\n",
       " 'dig',\n",
       " 'do',\n",
       " 'dob',\n",
       " 'doc',\n",
       " 'doe',\n",
       " 'dog',\n",
       " 'dogs',\n",
       " 'doh',\n",
       " 'don',\n",
       " 'dos',\n",
       " 'dot',\n",
       " 'dug',\n",
       " 'fog',\n",
       " 'hog',\n",
       " 'jog',\n",
       " 'log',\n",
       " 'tog',\n",
       " 'wog'}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "filter_unknown(one_edit_from(\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-short",
   "metadata": {},
   "source": [
    "This is a far more manageable list, and is more useful as well because we don't want to correct misspelled words into words that don't exist in the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-howard",
   "metadata": {},
   "source": [
    "To expand our search-space, we'll also include candidates that are two simple edits from the original word. This function, `two_edits_from` just stacks calls to `one_edit_from`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extended-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_edits_from(word):\n",
    "    # Find all words one edit away\n",
    "    first_edits = one_edit_from(word)\n",
    "\n",
    "    # Find all words one edit away from the first edits\n",
    "    second_edits = set()\n",
    "    for first_edit in first_edits:\n",
    "        second_edits.update(one_edit_from(first_edit))\n",
    "\n",
    "    all_edits = first_edits.union(second_edits)\n",
    "\n",
    "    return all_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vital-single",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ample', 'examine', 'example', 'examples', 'sample', 'trample'}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "filter_unknown(two_edits_from(\"example\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-petite",
   "metadata": {},
   "source": [
    "Tying it all together, we write the function `get_candidates` which accepts a word `w` and returns `w` if `w` is known, else all known words one edit from `w`. If there are no known words one edit from `w` then it returns all known words two edits from `w`. Failing that, it just returns `w`.\n",
    "\n",
    "This function is meant to represent `P(w|c)` as it returns the candidates most likely to be the intended word. However, it falsely assumes that any candidate one edit away from the original word is infinitely more likely to be intended than a word two edits away. This isn't always true. For example, if `w` is \"rember\" then our model thinks \"member\" is infinitely more likely than \"remember\" which is clearly not true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "intelligent-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(word):\n",
    "    # If the word is in the dictionary, no need to correct it\n",
    "    if word in DICTIONARY:\n",
    "        return [word]\n",
    "\n",
    "    # If we have words that are one edit away and in the dictionary, return those\n",
    "    first_edits = filter_unknown(one_edit_from(word))\n",
    "    if len(first_edits) != 0:\n",
    "        return first_edits\n",
    "\n",
    "    # If we have words that are two edits away and in the dictionary, return those\n",
    "    second_edits = filter_unknown(two_edits_from(word))\n",
    "    if len(second_edits) != 0:\n",
    "        return second_edits\n",
    "\n",
    "    # Otherwise, just return the original word as we couldn't find a candidate\n",
    "    return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'capture',\n",
       " 'compare',\n",
       " 'compere',\n",
       " 'composure',\n",
       " 'compute',\n",
       " 'computer',\n",
       " 'couture'}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "get_candidates(\"compture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-solid",
   "metadata": {},
   "source": [
    "## Evaluating P(c)\n",
    "To calculate the `p(c)` for any word `c`, we will use a large corpus of text and count the occurrences of `c` within the text and divide it by the total amount of words in the corpus. This will give us a good, if a bit crude, estimate of the probability of `c` showing up in our text. Our corpus will be a text file containing a large sample of ebooks from Project Gutenberg. We create a Counter that reads in all the words accumulates counts for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abroad-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_word_counts\n",
    "WORD_COUNTS = get_word_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "living-token",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2272721"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "WORD_COUNTS[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "endangered-scholar",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "WORD_COUNTS[\"monopoly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "completed-plaintiff",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "WORD_COUNTS[\"horticulture\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "revised-bhutan",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('the', 2272721),\n",
       " ('of', 1241239),\n",
       " ('and', 1239810),\n",
       " ('to', 1038194),\n",
       " ('a', 820395),\n",
       " ('in', 667929),\n",
       " ('i', 588076),\n",
       " ('that', 528540)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "WORD_COUNTS.most_common(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-memorial",
   "metadata": {},
   "source": [
    "We can now create the function `probability_of` which returns our estimate of `p(c)` given candidate word `c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "modern-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of(word):\n",
    "    return WORD_COUNTS[word] / sum(WORD_COUNTS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sublime-satin",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.05866879067049132"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "probability_of(\"the\") # Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "received-stanley",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3.639821819105502e-06"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "probability_of(\"monopoly\") # Not large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-champion",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "We can now generate candidates and evaluate their probability of occurring. From these ingredients we can define a simple function to return the candidate with the maximum probability, completing our spelling corrector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "inside-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    return max(get_candidates(word), key=probability_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "historic-cardiff",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "speling -> spelling\n",
      "computinga -> computing\n",
      "clasrom -> classroom\n",
      "ptyhon -> python\n",
      "prgrammin -> programming\n",
      "jonahtan -> jonahtan\n"
     ]
    }
   ],
   "source": [
    "misspellings = [\"speling\", \"computinga\", \"clasrom\", \"ptyhon\", \"prgrammin\", \"jonahtan\"]\n",
    "for misspelling in misspellings:\n",
    "    print(f\"{misspelling} -> {correct(misspelling)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-healthcare",
   "metadata": {},
   "source": [
    "And it works! Though it apparently doesn't like names, can you figure out why? (Hint: Think about how we're filtering out unknown words)"
   ]
  },
  {
   "source": [
    "## Conclusion\n",
    "I hope you found this notebook insightful into how spelling correction works at a base level. I've always found it interesting to see how technologies we use every day might work and put together this notebook to share that interest.\n",
    "\n",
    "Please reach out if you have any questions or suggestions!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "editorial-professional",
   "metadata": {},
   "source": [
    "## Attributions\n",
    "- The dictionary file was sourced from SCOWL's 12Dicts package: http://wordlist.aspell.net/12dicts/\n",
    "- The Gutenberg corpus was compiled from Shibamouli Lahiri's work: https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-nicaragua",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "These are the `utils` functions that we imported throughout the notebook. They generally just handle reading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "narrow-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def only_words(text: str):\n",
    "    return re.findall(r\"[^_\\W]+\", text.lower())\n",
    "\n",
    "\n",
    "def get_words():\n",
    "    with open(\"./datasets/dictionary.txt\") as reader:\n",
    "        return set(only_words(reader.read()))\n",
    "\n",
    "\n",
    "def get_word_counts():\n",
    "    with open(\"./datasets/corpus.txt\") as reader:\n",
    "        return Counter(only_words(reader.read()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}